batchsize: 512
batchsize_valid: 1024
seed: 0
epochs: 5
maxlen: 72

preprocessors:
  - lower
  - punct
  - number
  - hengzheng_mispell
  - keras
tokenizer: space
embedding:
  src:
    - glove
    - paragram
  model:
    word2vec
  params:
    size: 300
  standardize:
    False

optimizer:
  lr: 1e-3
model:
  embed:
    n_embed: 300
    freeze_embed: True
    position: false
    n_hidden: 0
    hidden_bn: False
    dropout: 0.2
  encoder:
    dropout: 0.2
    n_hidden: 128
    n_layers: 2
    out_scale: 2
    aggregator: max
  mlp:
    bn: True
    n_layers: 2
    n_hidden: 128
ensembler: avg
