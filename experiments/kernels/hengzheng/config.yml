batchsize: 512
batchsize_valid: 1024
seed: 0
epochs: 5
maxlen: 72
pos_weight: 1.

vocab:
  max_size: 95000
  min_count: 5
preprocessors:
  - lower
  - punct
  - number
  - hengzheng_misspell
  - keras
tokenizer: space
embedding:
  add_los:
    False
  finetune:
    False
  src:
    - glove
    - paragram
  model:
    word2vec
  params:
    alpha: 1e-4
    size: 300
  standardize:
    False

optimizer:
  name: adam
  params:
    lr: 0.001
  scheduler:
    ReduceLROnPlateau

ensembler:
  model: avg
  test_size: 0.1
